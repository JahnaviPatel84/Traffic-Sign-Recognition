# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KUA4ztKvhM-MDR9c_LWkyRLDZZVT7M_S
"""

import streamlit as st
import torch
import torch.nn.functional as F
import numpy as np
import cv2
from PIL import Image
from torchvision import transforms

# ==== Define Model ====
class TrafficSignCNN(torch.nn.Module):
    def __init__(self, num_classes=43):
        super().__init__()
        self.conv1 = torch.nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.conv2 = torch.nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = torch.nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.pool = torch.nn.MaxPool2d(2, 2)
        self.dropout = torch.nn.Dropout(0.5)
        self.fc1 = torch.nn.Linear(128 * 4 * 4, 512)
        self.fc2 = torch.nn.Linear(512, num_classes)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = self.pool(F.relu(self.conv3(x)))
        x = x.view(-1, 128 * 4 * 4)
        x = self.dropout(F.relu(self.fc1(x)))
        x = self.fc2(x)
        return x

# ==== Grad-CAM Utility ====
def generate_gradcam(model, image_tensor, class_idx, target_layer='conv3'):
    activations, gradients = [], []

    def forward_hook(module, input, output):
        activations.append(output)

    def backward_hook(module, grad_input, grad_output):
        gradients.append(grad_output[0])

    layer = getattr(model, target_layer)
    handle_f = layer.register_forward_hook(forward_hook)
    handle_b = layer.register_backward_hook(backward_hook)

    image_tensor = image_tensor.unsqueeze(0)
    output = model(image_tensor)
    model.zero_grad()
    output[0, class_idx].backward()

    handle_f.remove()
    handle_b.remove()

    act = activations[0].squeeze().detach().numpy()
    grad = gradients[0].squeeze().detach().numpy()
    weights = np.mean(grad, axis=(1, 2))

    cam = np.zeros(act.shape[1:], dtype=np.float32)
    for i, w in enumerate(weights):
        cam += w * act[i]

    cam = np.maximum(cam, 0)
    cam = cv2.resize(cam, (32, 32))
    cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)
    return cam

# ==== Class Labels ====
CLASS_ID_TO_NAME = {
    0: 'Speed limit (20km/h)', 1: 'Speed limit (30km/h)', 2: 'Speed limit (50km/h)',
    3: 'Speed limit (60km/h)', 4: 'Speed limit (70km/h)', 5: 'Speed limit (80km/h)',
    6: 'End of speed limit (80km/h)', 7: 'Speed limit (100km/h)', 8: 'Speed limit (120km/h)',
    9: 'No passing', 10: 'No passing > 3.5 tons', 11: 'Right-of-way at next intersection',
    12: 'Priority road', 13: 'Yield', 14: 'Stop', 15: 'No vehicles',
    16: 'No vehicles > 3.5 tons', 17: 'No entry', 18: 'General caution',
    19: 'Left curve', 20: 'Right curve', 21: 'Double curve', 22: 'Bumpy road',
    23: 'Slippery road', 24: 'Road narrows right', 25: 'Road work',
    26: 'Traffic signals', 27: 'Pedestrians', 28: 'Children crossing',
    29: 'Bicycles crossing', 30: 'Ice/snow', 31: 'Wild animals',
    32: 'End of all restrictions', 33: 'Turn right ahead', 34: 'Turn left ahead',
    35: 'Ahead only', 36: 'Straight or right', 37: 'Straight or left',
    38: 'Keep right', 39: 'Keep left', 40: 'Roundabout', 41: 'End of no passing',
    42: 'End of no passing > 3.5 tons'
}

# ==== Load Model ====
model = TrafficSignCNN()
model.load_state_dict(torch.load("models/traffic_sign_cnn.pth", map_location="cpu"))
model.eval()

# ==== Streamlit UI ====
st.title("ðŸš¦ Traffic Sign Recognition")
st.markdown("Upload an image of a traffic sign to see the prediction and heatmap.")

uploaded_file = st.file_uploader("Upload Image", type=["jpg", "png", "jpeg"])

if uploaded_file:
    image = Image.open(uploaded_file).convert("RGB")
    st.image(image, caption="Uploaded Image", use_column_width=True)

    # Transform image
    transform = transforms.Compose([
        transforms.Resize((32, 32)),
        transforms.ToTensor(),
        transforms.Normalize([0.5]*3, [0.5]*3)
    ])
    input_tensor = transform(image)

    # Prediction
    with torch.no_grad():
        output = model(input_tensor.unsqueeze(0))
        pred_class = output.argmax(dim=1).item()
        pred_name = CLASS_ID_TO_NAME[pred_class]

    st.success(f"**Prediction:** {pred_name}")

    # Grad-CAM
    cam = generate_gradcam(model, input_tensor, pred_class)
    image_np = input_tensor.permute(1, 2, 0).numpy()
    image_np = (image_np * 0.5 + 0.5).clip(0, 1)

    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)
    overlay = cv2.addWeighted(np.uint8(image_np * 255), 0.6, heatmap, 0.4, 0)

    st.image(overlay, caption="Grad-CAM Explanation", channels="BGR")